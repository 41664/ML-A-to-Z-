{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Neural Network \n",
    "\n",
    "We classify images based on the features we see in the images. Based on what features our brain sees, it processes and classifies and tells us what it is. CNN uses a probabilistic approach to classfiy objects that it sees. Yan LeCun is regarded as the father of CNNs. \n",
    "\n",
    "**Input**- An image\n",
    "**Output**- Probabilities for labels. \n",
    "\n",
    "### Here's how it works\n",
    "\n",
    "\n",
    "![steps](Steps.png)\n",
    "\n",
    "## Further Reading \n",
    "\n",
    "[Original Paper By Yann Lecun](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)\n",
    "\n",
    "[The Most popular introduction](https://pdfs.semanticscholar.org/450c/a19932fcef1ca6d0442cbf52fec38fb9d1e5.pdf) \n",
    "\n",
    "[WHy do they use a non linear function](https://arxiv.org/abs/1609.04112)\n",
    "\n",
    "[An analysis on Pooling operations](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf)\n",
    "\n",
    "[CNN visualization](http://www.cs.cmu.edu/~aharley/vis/conv/flat.html)\n",
    "\n",
    "## Concepts importnat and learnt in this \n",
    "\n",
    "1. **Convolution** \n",
    "       a. Convolution Kernel \n",
    "       b. Convolution padding and stride\n",
    "       c. Convolution accross volumes\n",
    "2. **Pooling**\n",
    "3. **Designing of some popular CNNs** like VGG-16, Alexnet and Le-Net5\n",
    "4. **Resnet**\n",
    "5. **Inception network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem- Training Dog vs Cat classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can download the dataset from the superdatascience website or Kaggle. \n",
    "path= \"../../Convolutional_Neural_Networks/dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten \n",
    "from keras.layers import Dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "# Adding the first convolution layer\n",
    "classifier.add(Convolution2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(64,64,3)))\n",
    "#input shape defines size of image and it is rescaled for uniform behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "#Adding the pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())\n",
    "# Adding the flattening layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the fully connected layer\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dense(units=1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2426s 303ms/step - loss: 0.4580 - acc: 0.7786 - val_loss: 0.5541 - val_acc: 0.7745\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 2467s 308ms/step - loss: 0.2833 - acc: 0.8783 - val_loss: 0.8459 - val_acc: 0.7528\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 2440s 305ms/step - loss: 0.1949 - acc: 0.9211 - val_loss: 1.0266 - val_acc: 0.7436\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 2244s 280ms/step - loss: 0.1476 - acc: 0.9423 - val_loss: 1.2780 - val_acc: 0.7405\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 2588s 324ms/step - loss: 0.1176 - acc: 0.9553 - val_loss: 1.3748 - val_acc: 0.7471\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 2333s 292ms/step - loss: 0.0963 - acc: 0.9638 - val_loss: 1.4784 - val_acc: 0.7492\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 2294s 287ms/step - loss: 0.0845 - acc: 0.9688 - val_loss: 1.5624 - val_acc: 0.7601\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 2279s 285ms/step - loss: 0.0737 - acc: 0.9737 - val_loss: 1.6461 - val_acc: 0.7390\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 2297s 287ms/step - loss: 0.0641 - acc: 0.9769 - val_loss: 1.5851 - val_acc: 0.7610\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 2647s 331ms/step - loss: 0.0592 - acc: 0.9793 - val_loss: 1.9120 - val_acc: 0.7383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9ab7ed4eb8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        path+'/training_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        path+'/test_set',\n",
    "        target_size=(64, 64), #dimension of image expected by CNN\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "classifier.fit_generator(\n",
    "        training_set, #Training set generator\n",
    "        steps_per_epoch=8000, # Number of images in our set\n",
    "        epochs=10, #My computer is really slow\n",
    "        validation_data=test_set, #The place from where it needs to validate\n",
    "        validation_steps=2000) #The number of images in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I know this model is overfitted but I can't afford to train more\n",
    "# You can always improve the layers by adding more convolution layer or fully connected layer \n",
    "# If you add another convolutional layer, you need to add another max pool layer \n",
    "# When you are adding a new layer, you do not need to add the input shape parameter \n",
    "# If you increase size of your images in your input images, you get more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=path+'single_prediction'\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image= image.load_img(path+'/cat_or_dog_1.jpg',target_size=(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image=image.img_to_array(test_image)\n",
    "#Converts image of 2 dimensions to image of 3 dimensions\n",
    "#In general the predict expects input in a batch even if batch size is 1 so we need to add 4 dimensions before predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image=np.expand_dims(test_image, axis=0, )\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "result=classifier.predict(test_image)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But what mapping does it correspond to \n",
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Will add more info here "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
